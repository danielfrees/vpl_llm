# This file is used to preprocess dataset, available for any HH-RLHF format datasets
import os
from dataclasses import dataclass, field
from typing import Optional, cast
from tqdm import tqdm
import random

from transformers import (
    HfArgumentParser,
)

import torch

from vpl_modules.train_llm_preference_model import (
    concatenate_datasets,
)

from vpl_modules.data_utils.data_processing import generate_embeddings_with_llm
from datasets import load_dataset

from copy import deepcopy

import numpy as np


@dataclass
class ScriptArguments:
    output_dir: Optional[str] = field(
        metadata={"help": "Directory where the new dataset will be stored."},
    )
    data_path: str = field(
        metadata={"help": "Directory where the original data is stored."}
    )
    data_subset: str = field(
        default="helpful",
        metadata={
            "help": "Which subset of the data to use. You can choose between"
                    "'helpful', or 'harmless'."
        },
    )
    data_split: str = field(
        default="test",
        metadata={
            "help": "Which split of the data to use. You can choose between"
                    "'train', or 'test'."
        },
    )
    dataset_size: int = field(
        default=0,
        metadata={"help": "The size of the subset of the data to use"},
    )
    model_type: str = field(
        default="none",
        metadata={
            "help": "You can choose between 'gpt2', 'llama', or 'none'."
        }
    )
    embed_dim: int = field(
        default=1024,
        metadata={
            "help": "Dimension of the embeddings generated by LLM."
        }
    )
    max_length: int = field(
        default=1024,
        metadata={
            "help": "Maximum token length of the inputs."
        }
    )
    with_embeddings: bool = field(
        default=True,
        metadata={
            "help": "Whether the embeddings are generated during pre-processing."
        }
    )
    synthetic_dataset: bool = field(
        default=False,
        metadata={
            "help": "Whether a synthetic dataset is used."
        }
    )
    other_subsets: str = field(default=None)
    survey_size: int = field(
        default=8,
        metadata={
            "help": "Size of survey question pool."
        }
    )
    context_length: int = field(
        default=8,
        metadata={
            "help": "(Maximum) context length."
        }
    )
    controversial_only: bool = field(
        default=True,
        metadata={
            "help": "Whether to only generate controversial data points."
        }
    )
    num_duplicates: int = field(
        default=1,
        metadata={
            "help": "Number of times each data point repeatedly appears in the dataset (with resampled context)."
        }
    )
    fixed_context_length: bool = field(
        default=False,
        metadata={
            "help": "Whether to fix the context to the maximum length."
        }
    )
    random_contexts: bool = field(
        default=False,
        metadata={
            "help": "Whether to include controversial pairs in context."
        }
    )


def generate_contexts(args, input_dataset, survey_dataset):
    # Generate context with survey question pool
    output_dir = os.path.join(args.output_dir, f"{args.model_type}", f"{args.data_subset}")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    if args.controversial_only:
        input_dataset = input_dataset.filter(lambda x: x['controversial'] == True)
    dataset_size = len(input_dataset)
    if args.data_split == 'train':
        K = args.num_duplicates  # repeat samples for K times
    else:
        K = 1
    dataset_list = list()

    def random_choice(max_context_length, survey_size):
        if max_context_length <= survey_size:
            from functools import reduce
            while True:
                if args.fixed_context_length:
                    context_length = max_context_length
                else:
                    if args.other_subsets == '84':
                        context_length = random.randint(1, max_context_length)
                    else:
                        context_length = random.randint(2, max_context_length)
                context_chosen_ids = np.random.choice(survey_size, context_length, replace=False)
                chosen_dataset = [d for idx, d in enumerate(survey_dataset) if idx in context_chosen_ids]
                if args.other_subsets != 'single':
                    return chosen_dataset, context_length
                satisfied_sets = list()
                for row in chosen_dataset:
                    satisfied_sets.append(set(row["satisfied_subset"]))
                if len(reduce(lambda x, y: x.intersection(y), satisfied_sets)) == 1:
                    return chosen_dataset, context_length
                elif context_length == survey_size:
                    raise ValueError("Please choose another random seed!")
        else:
            raise ValueError("Context length is larger than survey size!")

    for idx in range(K):
        output_dataset = deepcopy(input_dataset)
        context_lengths = list()
        contexts = list()
        for _ in tqdm(range(dataset_size)):  # iterate over all samples in original dataset
            row_contexts = list()

            context_dataset, context_length = random_choice(args.context_length, args.survey_size)
            context_lengths.append(context_length)
            for context_row in context_dataset:
                context_id = context_row["Index"]
                context_data = context_row
                if not args.with_embeddings:
                    row_contexts.append({
                        'original_id': context_id,
                        'chosen': context_data['chosen'],
                        'rejected': context_data['rejected'],
                    })
                else:
                    row_contexts.append({
                        'original_id': context_id,
                        'embedding_chosen': context_data['embeddings']['embedding_chosen'],
                        'embedding_rejected': context_data['embeddings']['embedding_rejected'],
                    })
            contexts.append(row_contexts)
        output_dataset = output_dataset.add_column("context_length", context_lengths)
        output_dataset = output_dataset.add_column("contexts", contexts)
        output_dataset.map()
        dataset_list.append(output_dataset)
    output = concatenate_datasets(dataset_list)
    output.to_json(os.path.join(output_dir, f"{args.data_split}.jsonl"))
    return output


if __name__ == "__main__":
    # default setting on HH-RLHF dataset, please iterate over data subsets and data splits
    seed = 0
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    parser = HfArgumentParser(ScriptArguments)
    script_args: ScriptArguments = parser.parse_args_into_dataclasses()[0]
    print(script_args)
    dataset = generate_embeddings_with_llm(script_args)
    if not script_args.random_contexts:
        survey_options = dataset.filter(lambda x: x['survey_options'] == True)
    else:
        survey_options = dataset.filter(lambda x: x['survey_options'] == True or x['survey_options'] == False)
    survey_ids = np.random.choice(range(len(survey_options)), script_args.survey_size, replace=False)
    print(survey_ids)
    if script_args.data_split == "train":
        survey_data = survey_options.filter(lambda example, idx: idx in survey_ids, with_indices=True)
        survey_data.to_json(os.path.join(script_args.data_path, script_args.data_subset, "survey_{}.jsonl".format(script_args.survey_size)))
    else:
        survey_data = load_dataset('json', data_files=os.path.join(script_args.data_path, script_args.data_subset, "survey_{}.jsonl".format(script_args.survey_size)))
        survey_data = survey_data['train']
    generate_contexts(script_args, dataset, survey_data)
